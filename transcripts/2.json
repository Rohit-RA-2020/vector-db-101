[
    {
        "text": " Last video, I laid out the structure of a neural network.",
        "start": 4.160000000000002,
        "end": 7.24,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=4"
    },
    {
        "text": " I'll give a quick recap here just so that it's fresh in our minds and then I have two main",
        "start": 7.56,
        "end": 11.58,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=7"
    },
    {
        "text": " goals for this video.",
        "start": 11.58,
        "end": 12.56,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=11"
    },
    {
        "text": " The first is to introduce the idea of gradient descent, which underlies not only how neural",
        "start": 13.08,
        "end": 17.66,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=13"
    },
    {
        "text": " networks learn, but how a lot of other machine learning works as well.",
        "start": 17.66,
        "end": 20.52,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=17"
    },
    {
        "text": " Then after that, we're going to dig in a little more to how this particular network performs",
        "start": 21.14,
        "end": 24.58,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=21"
    },
    {
        "text": " and what those hidden layers of neurons end up actually looking for.",
        "start": 25.080000000000002,
        "end": 27.84,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=25"
    },
    {
        "text": " As a reminder, our goal here is the classic example of handwritten digit recognition, the",
        "start": 29.000000000000004,
        "end": 34.76,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=29"
    },
    {
        "text": " Hello World of Neural Networks.",
        "start": 34.76,
        "end": 36.46,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=34"
    },
    {
        "text": " These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between",
        "start": 37.12,
        "end": 42.5,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=37"
    },
    {
        "text": " 0 and 1.",
        "start": 42.5,
        "end": 43.34,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=42"
    },
    {
        "text": " Those are what determine the activations of 784 neurons in the input layer of the network.",
        "start": 43.9,
        "end": 49.96,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=43"
    },
    {
        "text": " And then the activation for each neuron in the following layers is based on a weighted",
        "start": 51.08,
        "end": 55.44,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=51"
    },
    {
        "text": " sum of all the activations in the previous layer plus some special number called a bias.",
        "start": 55.44,
        "end": 60.82,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=55"
    },
    {
        "text": " Then you compose that sum with some other function, like the sigmoid squishification or",
        "start": 62.04,
        "end": 66.58,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=62"
    },
    {
        "text": " a rey-loo, the way that I walked through last video.",
        "start": 66.58,
        "end": 68.86,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=66"
    },
    {
        "text": " In total, given the somewhat arbitrary choice of two hidden layers here with 16 neurons each,",
        "start": 69.56,
        "end": 74.46,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=69"
    },
    {
        "text": " the network has about 13,000 weights and biases that we can adjust, and it's these values",
        "start": 74.98,
        "end": 80.74,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=74"
    },
    {
        "text": " that determine what exactly the network actually does.",
        "start": 80.74,
        "end": 84.22,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=80"
    },
    {
        "text": " Then what we mean when we say that this network classifies a given digit is that the brightest",
        "start": 84.86,
        "end": 89.68,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=84"
    },
    {
        "text": " of those 10 neurons in the final layer corresponds to that digit.",
        "start": 89.68,
        "end": 93.28,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=89"
    },
    {
        "text": " And remember, the motivation that we had in mind here for the layered structure was that",
        "start": 93.82,
        "end": 98.12,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=93"
    },
    {
        "text": " maybe the second layer could pick up on the edges and the third layer might pick up on",
        "start": 98.12,
        "end": 103.38,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=98"
    },
    {
        "text": " patterns like loops and lines, and the last one could just piece together those patterns",
        "start": 103.38,
        "end": 107.48,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=103"
    },
    {
        "text": " to recognize digits.",
        "start": 107.48,
        "end": 108.68,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=107"
    },
    {
        "text": " So here, we learn how the network learns.",
        "start": 109.4,
        "end": 112.18,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=109"
    },
    {
        "text": " What we want is an algorithm where you can show this network a whole bunch of training data,",
        "start": 112.68,
        "end": 117.28,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=112"
    },
    {
        "text": " which comes in the form of a bunch of different images of handwritten digits,",
        "start": 117.62,
        "end": 121.02,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=117"
    },
    {
        "text": " along with labels for what they're supposed to be, and it'll adjust those 13,000",
        "start": 121.28,
        "end": 126.18,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=121"
    },
    {
        "text": " weights and biases so as to improve its performance on the training data.",
        "start": 126.18,
        "end": 130.08,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=126"
    },
    {
        "text": " Hopefully, this layered structure will mean that what it learns generalizes to images beyond",
        "start": 130.72,
        "end": 135.92,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=130"
    },
    {
        "text": " that training data. And the way we test that is that after you train the network,",
        "start": 135.92,
        "end": 140.32,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=135"
    },
    {
        "text": " you show it more labeled data, that it's never seen before, and you see how accurately it classifies",
        "start": 140.74,
        "end": 145.72,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=140"
    },
    {
        "text": " those new images.",
        "start": 145.72,
        "end": 146.52,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=145"
    },
    {
        "text": " Fortunately for us, and what makes this such a common example to start with,",
        "start": 150.88,
        "end": 154.16,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=150"
    },
    {
        "text": " is that the good people behind the M-NIST database have put together a collection of tens of",
        "start": 154.46,
        "end": 159.0,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=154"
    },
    {
        "text": " thousands of handwritten digit images each one labeled with the numbers that they're supposed to be.",
        "start": 159.0,
        "end": 164.04,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=159"
    },
    {
        "text": " And it's provocative as it is to describe a machine as learning.",
        "start": 164.65999999999997,
        "end": 167.82,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=164"
    },
    {
        "text": " Once you actually see how it works, it feels a lot less like some crazy sci-fi premise,",
        "start": 168.38,
        "end": 172.36,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=168"
    },
    {
        "text": " and a lot more like, more like calculus exercise.",
        "start": 172.76,
        "end": 175.46,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=172"
    },
    {
        "text": " I mean, basically, it comes down to finding the minimum of a certain function.",
        "start": 176.18,
        "end": 179.84,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=176"
    },
    {
        "text": " Remember, conceptually, we're thinking of each neuron as being connected to all of the neurons",
        "start": 182.01999999999998,
        "end": 186.84,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=182"
    },
    {
        "text": " in the previous layer, and the weights in the weighted sum defining its activation",
        "start": 186.84,
        "end": 191.04,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=186"
    },
    {
        "text": " are kind of like the strengths of those connections.",
        "start": 191.04,
        "end": 193.96,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=191"
    },
    {
        "text": " And the bias is some indication of whether that neuron tends to be active or inactive.",
        "start": 194.64,
        "end": 198.96,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=194"
    },
    {
        "text": " And to start things off, we're just going to initialize all of those weights and biases",
        "start": 199.64,
        "end": 203.08,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=199"
    },
    {
        "text": " totally randomly. Needless to say, this network is going to perform pretty horribly on a given",
        "start": 203.08,
        "end": 208.08,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=203"
    },
    {
        "text": " training example, since it's just doing something random. For example, you feed in this image of a",
        "start": 208.08,
        "end": 212.92,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=208"
    },
    {
        "text": " three, and the output layer just looks like a mess. So what you do is you define a cost function,",
        "start": 212.92,
        "end": 219.2,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=212"
    },
    {
        "text": " a way of telling the computer, no, bad computer. That output should have activations which are zero",
        "start": 219.2,
        "end": 226.02,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=219"
    },
    {
        "text": " for most neurons, but one for this neuron. What you gave me is utter trash.",
        "start": 226.02,
        "end": 230.62,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=226"
    },
    {
        "text": " To say that a little more mathematically, what you do is add up the squares of the differences",
        "start": 231.52,
        "end": 236.48,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=231"
    },
    {
        "text": " between each of those trash output activations and the value that you want them to have.",
        "start": 236.48,
        "end": 241.28,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=236"
    },
    {
        "text": " And this is what we'll call the cost of a single training example. Notice, this sum is small when",
        "start": 241.78,
        "end": 248.04,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=241"
    },
    {
        "text": " the network confidently classifies the image correctly, but it's large when the network seems like",
        "start": 248.04,
        "end": 254.58,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=248"
    },
    {
        "text": " it doesn't really know what it's doing. So then what you do is consider the average cost over all",
        "start": 254.58,
        "end": 262.3,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=254"
    },
    {
        "text": " of the tens of thousands of training examples at your disposal. This average cost is our measure",
        "start": 262.3,
        "end": 269.2,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=262"
    },
    {
        "text": " for how lousy the network is and how bad the computer should feel. And that's a complicated thing.",
        "start": 269.2,
        "end": 274.58,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=269"
    },
    {
        "text": " Remember how the network itself was basically a function, one that takes in 784 numbers as inputs,",
        "start": 274.58,
        "end": 281.08,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=274"
    },
    {
        "text": " the pixel values, and spits out 10 numbers as its output. And in a sense it's parameterized by all",
        "start": 281.52,
        "end": 287.68,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=281"
    },
    {
        "text": " these weights and biases. Well the cost function is a layer of complexity on top of that. It takes",
        "start": 287.68,
        "end": 293.6,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=287"
    },
    {
        "text": " as its input, those 13,000 or so weights and biases, and it spits out a single number describing",
        "start": 293.6,
        "end": 299.76,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=293"
    },
    {
        "text": " how bad those weights and biases are. And the way it's defined depends on the network's behavior",
        "start": 299.76,
        "end": 305.58,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=299"
    },
    {
        "text": " over all the tens of thousands of pieces of training data. That's a lot to think about.",
        "start": 305.58,
        "end": 310.56,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=305"
    },
    {
        "text": " But just telling the computer what a crappy job it's doing isn't very helpful. You want to tell",
        "start": 312.02000000000004,
        "end": 316.86,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=312"
    },
    {
        "text": " it how to change those weights and biases so that it gets better. To make it easier, rather than",
        "start": 316.86,
        "end": 322.5,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=316"
    },
    {
        "text": " struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one",
        "start": 322.5,
        "end": 328.06,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=322"
    },
    {
        "text": " number as an input and one number as an output. How do you find an input that minimizes the value",
        "start": 328.06,
        "end": 334.54,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=328"
    },
    {
        "text": " of this function? Calcula students will know that you can sometimes figure out that minimum explicitly.",
        "start": 334.54,
        "end": 339.94,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=334"
    },
    {
        "text": " But that's not always feasible for really complicated functions. Certainly not in the 13,000 input",
        "start": 340.68,
        "end": 346.66,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=340"
    },
    {
        "text": " version of this situation for our crazy complicated neural network cost function. A more flexible",
        "start": 346.66,
        "end": 352.4,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=346"
    },
    {
        "text": " tactic is to start at any all input and figure out which direction you should step to make that",
        "start": 352.4,
        "end": 358.4,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=352"
    },
    {
        "text": " output lower. Specifically, if you can figure out the slope of the function where you are,",
        "start": 358.4,
        "end": 363.84,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=358"
    },
    {
        "text": " then shift to the left if that slope is positive and shift the input to the right if that slope is negative.",
        "start": 364.38,
        "end": 369.9,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=364"
    },
    {
        "text": " If you do this repeatedly, at each point checking the new slope and taking the appropriate step,",
        "start": 372.34,
        "end": 376.9,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=372"
    },
    {
        "text": " you're going to approach some local minimum of the function. And the image you might have in",
        "start": 376.9,
        "end": 381.54,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=376"
    },
    {
        "text": " mind here is a ball rolling down a hill. And notice, even for this really simplified single input",
        "start": 381.54,
        "end": 387.22,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=381"
    },
    {
        "text": " function, there are many possible valleys that you might land in, depending on which random",
        "start": 387.22,
        "end": 392.48,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=387"
    },
    {
        "text": " input you start at. And there's no guarantee that the local minimum you land in is going to be the",
        "start": 392.48,
        "end": 397.2,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=392"
    },
    {
        "text": " smallest possible value of the cost function. That's going to carry over to our neural network case",
        "start": 397.2,
        "end": 402.04,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=397"
    },
    {
        "text": " as well. And I also want you to notice how if you make your step sizes proportional to the slope,",
        "start": 402.04,
        "end": 407.46,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=402"
    },
    {
        "text": " then when the slope is flattening out towards the minimum, your steps get smaller and smaller,",
        "start": 408.12,
        "end": 412.42,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=408"
    },
    {
        "text": " and that kind of helps you from overshooting. Bumping up the complexity a bit,",
        "start": 412.74,
        "end": 417.42,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=412"
    },
    {
        "text": " imagine instead of function with two inputs and one output. You might think of the input space as",
        "start": 417.84,
        "end": 423.0,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=417"
    },
    {
        "text": " the xy plane and the cost function as being graphed as a surface above it. Now instead of asking",
        "start": 423.0,
        "end": 429.52,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=423"
    },
    {
        "text": " about the slope of the function, you have to ask which direction should you step in this input space",
        "start": 429.52,
        "end": 435.32,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=429"
    },
    {
        "text": " so as to decrease the output of the function most quickly. In other words, what's the downhill",
        "start": 435.32,
        "end": 441.1,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=435"
    },
    {
        "text": " direction? And again, it's helpful to think of a ball rolling down that hill. Those of you",
        "start": 441.1,
        "end": 447.06,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=441"
    },
    {
        "text": " familiar with multivariable calculus will know that the gradient of a function gives you the direction",
        "start": 447.06,
        "end": 453.18,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=447"
    },
    {
        "text": " of steepest ascent. Basically which direction should you step to increase the function most quickly?",
        "start": 453.18,
        "end": 458.66,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=453"
    },
    {
        "text": " Naturally enough, taking the negative of that gradient gives you the direction to step that",
        "start": 459.52,
        "end": 463.9,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=459"
    },
    {
        "text": " decreases the function most quickly. And even more than that, the length of this gradient vector",
        "start": 463.9,
        "end": 469.8,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=463"
    },
    {
        "text": " is actually an indication for just how steep that steepest slope is. Now if you're unfamiliar",
        "start": 469.8,
        "end": 475.32,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=469"
    },
    {
        "text": " with multivariable calculus and you want to learn more, check out some of the work that I did",
        "start": 475.32,
        "end": 479.02,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=475"
    },
    {
        "text": " for Khan Academy on the topic. Honestly though, all that matters for you and me right now",
        "start": 479.02,
        "end": 484.0,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=479"
    },
    {
        "text": " is that in principle there exists a way to compute this vector. This vector that tells you what",
        "start": 484.0,
        "end": 489.46,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=484"
    },
    {
        "text": " the downhill direction is and how steep it is. You'll be okay if that's all you know and you're not",
        "start": 489.46,
        "end": 494.72,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=489"
    },
    {
        "text": " rock solid on the details. Because if you can get that, the algorithm for minimizing the function",
        "start": 494.72,
        "end": 500.36,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=494"
    },
    {
        "text": " is to compute this gradient direction, then take a small step downhill and just repeat that over and",
        "start": 500.36,
        "end": 506.44,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=500"
    },
    {
        "text": " over. It's the same basic idea for a function that has 13,000 inputs instead of two inputs.",
        "start": 506.44,
        "end": 512.74,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=506"
    },
    {
        "text": " Imagine organizing all 13,000 weights and biases of our network into a giant column vector.",
        "start": 513.3,
        "end": 519.4,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=513"
    },
    {
        "text": " The negative gradient of the cost function is just a vector. It's some direction inside this",
        "start": 520.0,
        "end": 526.44,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=520"
    },
    {
        "text": " insanely huge input space that tells you which nudges to all of those numbers is going to",
        "start": 526.44,
        "end": 532.0,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=526"
    },
    {
        "text": " cause the most rapid decrease to the cost function. And of course, with our specially designed",
        "start": 532.0,
        "end": 537.5,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=532"
    },
    {
        "text": " cost function, changing the weights and biases to decrease it means making the output of the network",
        "start": 537.5,
        "end": 543.44,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=537"
    },
    {
        "text": " on each piece of training data look less like a random array of 10 values and more like an actual",
        "start": 543.44,
        "end": 549.26,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=543"
    },
    {
        "text": " decision that we want it to make. It's important to remember this cost function involves an average",
        "start": 549.26,
        "end": 554.44,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=549"
    },
    {
        "text": " over all of the training data. So if you minimize it, it means it's a better performance on all of",
        "start": 554.44,
        "end": 560.28,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=554"
    },
    {
        "text": " those samples. The algorithm for computing this gradient efficiently, which is effectively the heart",
        "start": 560.28,
        "end": 568.5,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=560"
    },
    {
        "text": " of how a neural network learns is called back propagation. And it's what I'm going to be talking",
        "start": 568.5,
        "end": 573.0,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=568"
    },
    {
        "text": " about next video. There, I really want to take the time to walk through what exactly happens to",
        "start": 573.0,
        "end": 578.6,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=573"
    },
    {
        "text": " each weight and each bias for a given piece of training data, trying to give an intuitive feel for",
        "start": 578.6,
        "end": 583.74,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=578"
    },
    {
        "text": " what's happening beyond the pile of relevant calculus and formulas. Right here, right now, the",
        "start": 583.74,
        "end": 589.12,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=583"
    },
    {
        "text": " main thing I want you to know, independent of implementation details, is that what we mean when",
        "start": 589.12,
        "end": 594.14,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=589"
    },
    {
        "text": " we talk about a network learning is that it's just minimizing a cost function. And notice,",
        "start": 594.14,
        "end": 599.56,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=594"
    },
    {
        "text": " one consequence of that is that it's important for this cost function to have a nice smooth output",
        "start": 599.78,
        "end": 604.48,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=599"
    },
    {
        "text": " so that we can find a local minimum by taking little steps downhill. This is why, by the way,",
        "start": 604.48,
        "end": 610.34,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=604"
    },
    {
        "text": " artificial neurons have continuously ranging activations, rather than simply being active or",
        "start": 610.56,
        "end": 615.52,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=610"
    },
    {
        "text": " inactive in a binary way, the way that biological neurons are. This process of repeatedly nudging",
        "start": 615.58,
        "end": 622.16,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=615"
    },
    {
        "text": " an input of a function by some multiple of the negative gradient is called gradient descent.",
        "start": 622.16,
        "end": 626.74,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=622"
    },
    {
        "text": " It's a way to converge toward some local minimum of a cost function, basically a valley in this graph.",
        "start": 627.26,
        "end": 632.48,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=627"
    },
    {
        "text": " I'm still showing the picture of a function with two inputs, of course, because nudges in a 13,000",
        "start": 633.38,
        "end": 638.14,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=633"
    },
    {
        "text": " dimensional input space are a little hard to wrap your mind around, but there is actually a nice",
        "start": 638.14,
        "end": 642.34,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=638"
    },
    {
        "text": " non-spatial way to think about this. Each component of the negative gradient tells us two things.",
        "start": 642.34,
        "end": 648.38,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=642"
    },
    {
        "text": " The sign, of course, tells us whether the corresponding component of the input vector should be",
        "start": 648.96,
        "end": 653.94,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=648"
    },
    {
        "text": " nudged up or down. But importantly, the relative magnitudes of all these components kind of tells you",
        "start": 653.94,
        "end": 660.96,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=653"
    },
    {
        "text": " which changes matter more. You see, in our network, an adjustment to one of the weights might have a",
        "start": 660.96,
        "end": 669.2,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=660"
    },
    {
        "text": " much greater impact on the cost function than the adjustment to some other weight.",
        "start": 669.2,
        "end": 673.02,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=669"
    },
    {
        "text": " Some of these connections just matter more for our training data.",
        "start": 674.5,
        "end": 678.12,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=674"
    },
    {
        "text": " So a way that you can think about this gradient vector of our mind-warpingly massive cost",
        "start": 679.24,
        "end": 683.46,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=679"
    },
    {
        "text": " function is that it encodes the relative importance of each weight and bias,",
        "start": 683.46,
        "end": 688.16,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=683"
    },
    {
        "text": " that is, which of these changes is going to carry the most bang for your buck.",
        "start": 688.72,
        "end": 692.3,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=688"
    },
    {
        "text": " This really is just another way of thinking about direction. To take a simpler example,",
        "start": 693.64,
        "end": 698.38,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=693"
    },
    {
        "text": " if you have some function with two variables as an input and you compute that it's gradient at",
        "start": 698.38,
        "end": 703.68,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=698"
    },
    {
        "text": " some particular point comes out as 3-1. Then on the one hand, you can interpret that as saying that",
        "start": 703.68,
        "end": 710.36,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=703"
    },
    {
        "text": " when you're standing at that input, moving along this direction increases the function",
        "start": 710.36,
        "end": 714.76,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=710"
    },
    {
        "text": " most quickly. That when you graph the function above the plane of input points, that vector is what's",
        "start": 714.76,
        "end": 720.54,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=714"
    },
    {
        "text": " giving you the straight uphill direction. But another way to read that is to say that changes to",
        "start": 720.54,
        "end": 725.74,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=720"
    },
    {
        "text": " this first variable have three times the importance as changes to the second variable,",
        "start": 725.8,
        "end": 730.38,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=725"
    },
    {
        "text": " that at least in the neighborhood of the relevant input, nudging the x-value carries a lot more bang",
        "start": 730.7,
        "end": 736.22,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=730"
    },
    {
        "text": " for your buck. Alright, let's zoom out and sum up where we are so far. The network itself is",
        "start": 736.22,
        "end": 744.18,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=736"
    },
    {
        "text": " this function with 784 inputs and 10 outputs, defined in terms of all of these weighted sums.",
        "start": 744.18,
        "end": 750.0,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=744"
    },
    {
        "text": " The cost function is a layer of complexity on top of that. It takes the 13,000 weights and biases",
        "start": 750.64,
        "end": 756.42,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=750"
    },
    {
        "text": " as inputs and spits out a single measure of laziness based on the training examples.",
        "start": 756.42,
        "end": 761.7,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=756"
    },
    {
        "text": " And the gradient of the cost function is one more layer of complexity still. It tells us what nudges",
        "start": 762.44,
        "end": 769.0,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=762"
    },
    {
        "text": " to all of these weights and biases cause the fastest change to the value of the cost function,",
        "start": 769.0,
        "end": 774.1,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=769"
    },
    {
        "text": " which you might interpret as saying which changes to which weights matter the most.",
        "start": 774.38,
        "end": 777.76,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=774"
    },
    {
        "text": " So, when you initialize the network with random weights and biases and adjust them many times based",
        "start": 782.58,
        "end": 787.94,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=782"
    },
    {
        "text": " on this gradient descent process, how well does it actually perform on images that it's never seen",
        "start": 787.94,
        "end": 792.82,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=787"
    },
    {
        "text": " before? Well, the one that I've described here with the two hidden layers of 16 neurons each",
        "start": 792.82,
        "end": 798.02,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=792"
    },
    {
        "text": " chosen mostly for aesthetic reasons, well, it's not bad. It classifies about 96% of the new images",
        "start": 798.02,
        "end": 804.76,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=798"
    },
    {
        "text": " that it sees correctly. And honestly, if you look at some of the examples that it messes up on,",
        "start": 804.76,
        "end": 809.86,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=804"
    },
    {
        "text": " you kind of feel compelled to cut it a little slack.",
        "start": 810.24,
        "end": 812.34,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=810"
    },
    {
        "text": " Now, if you play around with the hidden layer structure and make a couple tweaks,",
        "start": 815.9000000000001,
        "end": 819.1,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=815"
    },
    {
        "text": " you can get this up to 98%. And that's pretty good. It's not the best. You can certainly get",
        "start": 819.5,
        "end": 824.9,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=819"
    },
    {
        "text": " better performance by getting more sophisticated than this plain vanilla network. But given how daunting",
        "start": 824.9,
        "end": 830.32,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=824"
    },
    {
        "text": " the initial task is, I just think there's something incredible about any network doing this well",
        "start": 830.32,
        "end": 835.38,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=830"
    },
    {
        "text": " on images that it's never seen before, given that we never specifically told it what patterns to look for.",
        "start": 835.38,
        "end": 841.02,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=835"
    },
    {
        "text": " Originally, the way that I motivated this structure was by describing a hope that we might have,",
        "start": 842.52,
        "end": 847.24,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=842"
    },
    {
        "text": " that the second layer might pick up on little edges, that the third layer would piece together",
        "start": 847.68,
        "end": 851.88,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=847"
    },
    {
        "text": " those edges to recognize loops and longer lines, and that those might be piece together to recognize",
        "start": 851.88,
        "end": 856.7,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=851"
    },
    {
        "text": " digits. So, is this what our network is actually doing? Well, for this one at least, not at all.",
        "start": 856.7,
        "end": 864.4,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=856"
    },
    {
        "text": " Remember how last video we looked at how the weights of the connections from all of the neurons in",
        "start": 864.82,
        "end": 869.3,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=864"
    },
    {
        "text": " the first layer to a given neuron in the second layer can be visualized as a given pixel pattern",
        "start": 869.3,
        "end": 874.7,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=869"
    },
    {
        "text": " that that second layer neuron is picking up on? Well, when we actually do that, for the weights",
        "start": 874.7,
        "end": 880.0,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=874"
    },
    {
        "text": " associated with these transitions from the first layer to the next, instead of picking up on",
        "start": 880.0,
        "end": 885.06,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=880"
    },
    {
        "text": " isolated little edges here and there, they look, well, almost random, just put some very loose",
        "start": 885.06,
        "end": 892.18,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=885"
    },
    {
        "text": " patterns in the middle there. It would seem that in the unfathomably large 13,000-dimensional space",
        "start": 892.18,
        "end": 898.14,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=892"
    },
    {
        "text": " of possible weights and biases, our network found itself a happy little local minimum that,",
        "start": 898.14,
        "end": 902.94,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=898"
    },
    {
        "text": " despite successfully classifying most images, doesn't exactly pick up on the patterns that we might",
        "start": 903.08,
        "end": 908.12,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=903"
    },
    {
        "text": " have hoped for. And to really drive this point home, watch what happens when you input a random",
        "start": 908.12,
        "end": 913.38,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=908"
    },
    {
        "text": " image. If the system was smart, you might expect it to either feel uncertain, maybe, not really",
        "start": 913.38,
        "end": 919.28,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=913"
    },
    {
        "text": " activating any of those 10 output neurons or activating them all evenly. But instead, it confidently",
        "start": 919.28,
        "end": 925.26,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=919"
    },
    {
        "text": " gives you some nonsense answer, as if it feels as sure that this random noise is a 5, as it does",
        "start": 925.26,
        "end": 931.38,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=925"
    },
    {
        "text": " that an actual image of a 5 is a 5. Frees differently? Even if this network can recognize digits pretty",
        "start": 931.38,
        "end": 938.0,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=931"
    },
    {
        "text": " well, it has no idea how to draw them. A lot of this is because it's such a tightly constrained",
        "start": 938.0,
        "end": 944.3,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=938"
    },
    {
        "text": " training setup. I mean, put yourself in the network's shoes here. From its point of view,",
        "start": 944.3,
        "end": 949.04,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=944"
    },
    {
        "text": " the entire universe consists of nothing but clearly defined unmoving digits centered in a tiny grid,",
        "start": 949.44,
        "end": 954.92,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=949"
    },
    {
        "text": " and its cost function just never gave it any incentive to be anything but utterly confident in",
        "start": 955.5,
        "end": 960.38,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=955"
    },
    {
        "text": " its decisions. So with this is the image of what those second layer neurons are really doing,",
        "start": 960.38,
        "end": 965.12,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=960"
    },
    {
        "text": " you might wonder why I would introduce this network with the motivation of picking up on edges",
        "start": 965.12,
        "end": 969.36,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=965"
    },
    {
        "text": " and patterns. I mean, that's just not at all what it ends up doing. Well, this is not meant to be our",
        "start": 969.36,
        "end": 975.12,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=969"
    },
    {
        "text": " end goal, but instead a starting point. Frankly, this is old technology, the kind researched in the 80s",
        "start": 975.12,
        "end": 980.88,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=975"
    },
    {
        "text": " and 90s. And you do need to understand it before you can understand more detailed modern variants,",
        "start": 980.88,
        "end": 985.9,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=980"
    },
    {
        "text": " and it clearly is capable of solving some interesting problems. But the more you dig in to what those",
        "start": 986.32,
        "end": 991.3,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=986"
    },
    {
        "text": " hidden layers are really doing, the less intelligent it seems. Shifting the focus for a moment from",
        "start": 991.3,
        "end": 1000.16,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=991"
    },
    {
        "text": " how networks learn to how you learn, that'll only happen if you engage actively with the material",
        "start": 1000.16,
        "end": 1005.54,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1000"
    },
    {
        "text": " here somehow. One pretty simple thing that I want you to do is just pause right now and think",
        "start": 1005.54,
        "end": 1011.48,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1005"
    },
    {
        "text": " deeply for a moment about what changes you might make to this system and how it perceives images",
        "start": 1011.48,
        "end": 1017.04,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1011"
    },
    {
        "text": " if you wanted it to better pick up on things like edges and patterns. But better than that,",
        "start": 1017.04,
        "end": 1022.58,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1017"
    },
    {
        "text": " to actually engage with the material, I highly recommend the book by Michael Neilsson on deep learning",
        "start": 1022.86,
        "end": 1028.2,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1022"
    },
    {
        "text": " and neural networks. In it, you can find the code and the data to download and play with for this",
        "start": 1028.2,
        "end": 1033.82,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1028"
    },
    {
        "text": " exact example, and the book will walk you through step by step what that code is doing. What's awesome",
        "start": 1033.82,
        "end": 1039.76,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1033"
    },
    {
        "text": " is that this book is free and publicly available. So if you do get something out of it, consider joining",
        "start": 1039.76,
        "end": 1045.1,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1039"
    },
    {
        "text": " me in making a donation towards Neilsson's efforts. I've also linked a couple other resources that I",
        "start": 1045.1,
        "end": 1050.3,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1045"
    },
    {
        "text": " like a lot in the description, including the phenomenal and beautiful blog posts by Chris Ola and",
        "start": 1050.3,
        "end": 1055.3,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1050"
    },
    {
        "text": " the articles in Distill. To close things off here for the last few minutes, I want to jump back into",
        "start": 1055.3,
        "end": 1061.56,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1055"
    },
    {
        "text": " a snippet of the interview that I had with Lisha Lee. You might remember her from the last video,",
        "start": 1061.56,
        "end": 1065.86,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1061"
    },
    {
        "text": " she did her PhD work in deep learning. And in this little snippet, she talks about two recent papers",
        "start": 1066.06,
        "end": 1070.8,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1066"
    },
    {
        "text": " that really dig into how some of the more modern image recognition networks are actually learning.",
        "start": 1070.8,
        "end": 1075.84,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1070"
    },
    {
        "text": " Just to set up where we were in the conversation, the first paper took one of these particularly deep",
        "start": 1076.16,
        "end": 1080.68,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1076"
    },
    {
        "text": " neural networks that's really good at image recognition. And instead of training it on a properly",
        "start": 1080.68,
        "end": 1085.28,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1080"
    },
    {
        "text": " labeled data set, it shuffled all of the labels around before training. Obviously, the testing",
        "start": 1085.28,
        "end": 1090.34,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1085"
    },
    {
        "text": " accuracy here was going to be no better than random since everything's just randomly labeled.",
        "start": 1090.34,
        "end": 1094.68,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1090"
    },
    {
        "text": " But it was still able to achieve the same training accuracy as you would on a properly labeled",
        "start": 1095.14,
        "end": 1100.42,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1095"
    },
    {
        "text": " data set. Basically, the millions of weights for this particular network were enough for it to",
        "start": 1100.42,
        "end": 1105.92,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1100"
    },
    {
        "text": " just memorize the random data, which kind of raises the question for whether minimizing this cost",
        "start": 1105.92,
        "end": 1110.98,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1105"
    },
    {
        "text": " function actually corresponds to any sort of structure in the image, or is it just, you know,",
        "start": 1110.98,
        "end": 1116.08,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1110"
    },
    {
        "text": " memorization? I always the entire data set of what the correct classification is. And so a couple",
        "start": 1116.36,
        "end": 1121.02,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1116"
    },
    {
        "text": " of half a year later at ICML this year, there was not exactly rebuttal paper that addressed some",
        "start": 1121.02,
        "end": 1127.9,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1121"
    },
    {
        "text": " aspects of like, hey, actually, these networks are doing something a little bit smarter than that.",
        "start": 1127.9,
        "end": 1132.22,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1127"
    },
    {
        "text": " If you look at that accuracy curve, if you were just training on a random data set, that curves",
        "start": 1132.28,
        "end": 1139.94,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1132"
    },
    {
        "text": " have went down very, you know, very slowly in almost kind of a linear fashion. So you're really",
        "start": 1139.94,
        "end": 1146.18,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1139"
    },
    {
        "text": " struggling to find that local minimum of possible, you know, the right weights that would get you",
        "start": 1146.18,
        "end": 1151.54,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1146"
    },
    {
        "text": " that accuracy. Whereas if you're actually training on a structure data set, one that has the",
        "start": 1151.54,
        "end": 1155.38,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1151"
    },
    {
        "text": " right labels, you know, you fiddle around a little bit in the beginning, but then you kind of dropped",
        "start": 1155.38,
        "end": 1159.54,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1155"
    },
    {
        "text": " very fast to get to that accuracy level. And so in some sense, it was easier to find that local",
        "start": 1159.54,
        "end": 1167.46,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1159"
    },
    {
        "text": " maxima. And so it was also interesting about that is it brings into light another paper from actually",
        "start": 1167.46,
        "end": 1173.3,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1167"
    },
    {
        "text": " a couple of years ago, which has a lot more simplifications about the network layers, but one of the",
        "start": 1173.3,
        "end": 1179.74,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1173"
    },
    {
        "text": " results was saying how if you look at the optimization landscape, the local minima that these",
        "start": 1179.74,
        "end": 1185.5,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1179"
    },
    {
        "text": " networks tend to learn are actually of equal quality. So in some sense, if your data set is structure,",
        "start": 1185.5,
        "end": 1191.94,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1185"
    },
    {
        "text": " you should be able to find that much more easily.",
        "start": 1192.16,
        "end": 1194.28,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1192"
    },
    {
        "text": " My thanks, as always, to those of you supporting on Patreon. I've said before just what a game",
        "start": 1198.24,
        "end": 1202.88,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1198"
    },
    {
        "text": " change in Patreon is, but these videos really would not be possible without you. I also want to give",
        "start": 1202.88,
        "end": 1208.2,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1202"
    },
    {
        "text": " a special thanks to the VC firm Amplify Partners in their support of these initial videos in the series.",
        "start": 1208.2,
        "end": 1213.16,
        "url": "https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&t=1208"
    }
]